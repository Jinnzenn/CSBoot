# 机器学习

## 什么是机器学习？
整体的流程

### 定义

- Arthur Samuel described it as: "the field of study that gives computers the ability to learn without being explicitly programmed." 
- Tom Mitchell provides a more modern definition: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
- 经过某一测试方法表明，计算机能够通过学习和累积经验，更好地完成某项任务

### 分类

- 机器学习
- 表示学习
- 深度学习
- 神经网络

### 概览

- 任务目标

	- 回归问题？分类问题？......

- 数据概览

	- 数据量
	- 数据的代表性
	- 数据的质量，有无缺失等等
	- 数据之间的关系

- 数据预处理

	- 数据清洗

		- 数据集的缺失数据补全

			- 中位数？
			- 平均值？
			- 删除？

		- 数据集的数字化

	- 数据增强

		- 原因：数据量不足，模型适应性可能不够好
		- 对于图像数据，数据增强包括图像旋转、平移、颜色变换、裁剪，仿射变换等等

	- 数据归一化 normalization

		- 均值标准化

			- 高斯核函数的SVM
			- 线性回归算法
			- PCA 主成分分析

	- 数据集的划分

		- 训练集、验证集、测试集

### 建模过程
Data (training set) -- Alg -- Model (with function/hypothesis)

- 模型选用

	- 根据问题的类型，选择合适的Alg 算法，也可以说合适的模型

		- 经验性的工作
		- 数学直觉上的指导

	- 模型的结构

		- 线性模型
		- 神经网络

	- 假设函数

		- 线性单元
		- 非线性单元

			- sigmoid
			- ReLU

- 参数学习

	- 参数估计
求最优参数

		- 参数初始化
		- 学习准则

			- 求连续值

				- 结构风险最小准则 SRM
				- 期望风险最小准则 ERM

			- 求离散值

				- 最大似然估计 MLE
				- 最大后验估计 MAP
贝叶斯线性回归

		- 代价函数 cost function
也叫
损失函数 loss function

			- 目标：实现学习准则
不断迭代以最小化minimaze代价函数，代价函数值最小，模型和训练集数据的拟合程度最高（这一阶段仅就训练集拟合得好）
			- 结合数据的特点设计代价函数
建立反馈，调整某一算法中的各项参数，完善并优化模型
代价函数 cost function

				- 均方差函数

					- 特点：必有全局最优
					- 优点：
					- 缺点：收敛速度后期慢

				- 交叉熵函数

					- 特点：

		- 优化算法

			- 正规方程法求方程解析解 
normal equations method

				- 优点：无须设置学习率，无须迭代非常多次。不用对输入数据做归一化处理
				- 缺点：但样本数量大时，计算缓慢

					- O(n^3)
					- 大于一万时不用

				- 缺点：非线性建模没有解析解
				- 缺点：解析通式中的X't * X 可能不可逆

					- 可能原因1：使用的某些特征之间具有线性关系

						- 排除

					- 可能原因2：使用了过多的特征变量而样本数量小于特征变量

						- 减去一些特征变量
						- 采用正则化方法，如果正则化参数大于0，则解析通式中矩阵将可逆

			- 基本方法

				- 轮廓图 conour graph/figure

					- 直观，但太慢

				- 梯度下降法 Gradient Descent

					- 找到局部最小/全局最小值
					- 同步更新所有参数 parameter

						- O (kn^2)

					- 学习率 learning rate 可以保持不变
梯度下降速度会自动减缓

				- 小批量梯度下降
				- 学习虑衰减
				- 梯度方向优化

			- 进阶方法
高等数值计算

				- 特点：相比Gradient Descent不用选择学习率a，自动校正a
				- 共轭梯度法
Conjugate Descent

					- 参数向量的维度应 >= 2

				- BFGS
				- L-BFGS

	- 超参数调优

		- 网格搜索
		- 随机搜索
		- 贝叶斯优化
		- 动态随机分配

- 得到决策函数 decision function

### 诊断过程
dignostic

- 评估假设函数（决策函数）
evaluating hypothesis

	- 使用验证集评估

		- 数据集的切分方式：
训练集 : 交叉验证集：测试集 = 6 : 2 ：2

			- 误差函数，和代价函数不一样，但是很类似
用来计算训练误差、泛化误差，然后判断偏差和方差，然后判断模型的拟合拟合情况

		- 训练集train set，训练每种模型，使训练误差 train error 最小
		- 验证集 cross validation，验证每种模型的泛化误差 general error，评估泛化能力，同时调整了参数d（控制多项式阶），选择最优模型
		- 测试集 test set，测试模型的泛化能力
		- 误差函数，和代价函数不一样，但是很类似
用来计算训练误差、泛化误差，然后判断偏差和方差，然后判断模型的拟合拟合情况

	- 使用评估指标

		- 根据问题本身选择合适的评估指标

			- 回归问题，使用交叉验证，计算准确率
			- 分类问题，使用预测率和召回率的调和值 F_1

- 诊断欠拟合和过拟合
dignose underfiting and overfitting

	- 欠拟合-高偏差

		- 原因：
假设模型太简单
		- 表现：
训练误差高，交叉验证集、测试集的泛化误差都很高
		- 学习曲线表现为，随着样本规模的增大，泛化误差和训练误差逐渐【水平】保持在高位，且相互逐渐接近，差距很小

	- 过拟合-高方差

		- 原因：
1、特征变量过多，或者说拟合模型选用的不对
2、样本数量太少
		- 表现：
训练误差低，但交叉验证集的泛化误差高
		- 学习曲线表现为，随着样本规模的增大，泛化误差刚开始一直在高位，训练误差也逐渐加大，相互逐渐接近
		- 可以转为恰拟合
		- 说明模型对训练集的适应很好，但不能适应新加入的数据，泛化能力差

	- 恰拟合

		- 表现：
训练误差缓步升高，泛化误差缓步下降最后保持【稳定差距】

			- 【稳定差距】就是随机误差

- 机器学习诊断方法
machine learning dignostic

	- 一般来说，大型神经网络+正则化方法往往比小型神经网络拟合的要好，但同时，计算量也比较大

- 得到模型优化思路

### 调试过程
debugging

- 一般思路

	- 使用更多的样本数据
	- 减少特征变量
	- 增加特征变量
	- 构造多项式特征变量
	- 调整正则化参数

- 欠拟合 underfitting
过拟合 overfitting
调试技术

	- 如果模型选择的特征变量包含了足够预测目标函数的信息，那么即使选用是较为简单的模型，随着数据的增加，也能获得高性能的模型。也就是训练误差和泛化误差都小，也就是恰拟合。

		- 如果选择的特征变量不具备这样的条件，盲目增加数据量是无益的，不能帮助生成高性能的模型。

	- 欠拟合-高偏差

		- 解决方法：
1、明确是高偏差时（而不是训练不完善），增加样本数量是没有用的，应该更换数学模型
2、通过增加特征或者构造特征来修改模型
3、正则化技术，使用较小的正则化参数
4、调整超参数hyparameter 比如隐藏层数量，层的单元数量

	- 过拟合-高方差

		- 解决方法：
1、使用模型选择算法或者人工地去除不必要的特征
2、增加样本数量
3、正则化技术，使用较大的正则化参数，变相降低模型参数权重的影响

- 正则化技术
regularization

	- 正则化参数的调整

		- 数学理解

			- 梯度下降方法角度

				- 提示：样本数量m、正则化参数和学习率组成式子的关系

			- 方程解析解角度

		- 正则化参数过大，各参数的权重都过小，模型接近线性，高偏差，欠拟合
		- 正则化参数过小，起不到纠正过拟合的效果
		- 正则化参数从较小的数值开始，以2倍速度增长，训练不同正则化参数下单模型，比较各个模型的验证误差，验证误差最小的正则化参数为最优参数

	- 典型的正则化方法：
代价函数中增加正则化项

		- 线性回归的正则化
		- logistic回归的正则化
		- 神经网络的正则化

	- 权重衰减
	- 随机失活
	- 提前停止
	- 其他正则化方法

		- 随机梯度下降，dropout，权重噪音，激活噪音，数据增强，这些都是深度学习中常用的正则化算子

- 得到 目标函数

	- 拟合良好
	- 泛化良好

		- 泛化：假设模型能够应用到新样本的能力

### 大规模机器学习的技巧
Large Scale Machine Learning

- 特征的调整
- 离线学习（批量学习）和在线学习（持续学习）
- 计算量的控制

	- 1、随机梯度下降法
2、小批量梯度下降
3、随机梯度下降收敛
	- 映射化简和数据并行

## 机器学习的分类

为了解决某一类问题，而有相应的模型

### 模型分类

给予结构化的数据，且数据带有标签。机器学习的目标是能够找出其中的数据规律，并能够用标签来评估

- 关键在于给出了 right answer

	- 回归问题

		- 从离散的数值预测连续的数值

	- 分类问题

		- 二元分类
		- 多元分类

			- 决策边界

- 线性分类模型

	- 特点：相比于神经网络
优点：
缺点：当原始输入特征数很多时，用新增特征组合来实现非线性决策边界，新增特征组合极多，计算量非常大
	- 分类

		- 朴素贝叶斯
分类器
		- logistic回归

			- 原因/用法：
为了解决分类问题

				- 线性回归算法不适用于分类问题
				- 模型中需要引入非线性属性
				- 二元分类，多元分类等问题可以使用logistic、神经网络等

			- 特点：
和线性回归相比较

				- 输出值位于[0,1]

					- 使用 sigmoid function(= logistic funciton)实现，同神经网络已经有点相似

						- sigmoid函数的特性：
当x<-4 or x>4 后，已经逼近-1 or +1

					- 输出值的实际意义是 y = 1的概率
h_theta(x) = P( y = 1 | x ; theta)

			- 优点：
基本效果不错
缺点：
不好处理特征变量极多的情况，对于高阶多项式的计算量会很大，梯度下降很慢
			- 二元分类的logistic算法

			  logistic回归算法，其实不算回归算法，仅仅是沿用了历史名称

				- 建模

					- 假设函数

						- 建立规律假设hypothesis，需要判断分类问题大概由哪种多项式方程可以拟合
						- 函数不同，参数取值不同，形成的划分不同，术语叫
决策边界 decision bound

					- 参数学习

						- 参数初始化
						- 学习准则
						- 代价函数

							- 使用代价函数（使用训练集中样本数据构造），反馈调整模型参数，取得全局最优值
							- 代价函数采用交叉熵
 y*logy - (1-y)*log(1-y)

								- 采用平方误差函数作为代价函数将会出现多个局部最优值，因此采用其他的函数

						- 优化算法

				- 诊断

					- 评估假设函数

						- 评估指标：F_1

					- 泛化能力

						- 偏差与方差
						- 欠拟合和过拟合

				- 调试

					- 正则化

						- 梯度下降
						- 更高级的优化方法

			- 多元分类的logistic回归
一对多算法
one-versus-all classification
或称
一对余算法
one-versus-rest classifcation

				- 如果要输出i个维度的参数，i次使用二元分类方法，这样可以是i个方向上拥有拟合特性的i个模型。代入样本数据后，哪个模型得出的值最高，样本即对应哪个维度

		- softmax回归
		- 感知器
		- 支持向量机 SVM
supported vector machine

			- 如何选择带核或者不带核？ 看特征数量和样本数量
			- 优点：SVM是一个凸优化问题，一定可以得到全局最优解，神经网络可能得到局部最优解
			- 大间隔分类器
线性核函数 linear kernel

				- 原因/用法：用于拟合线性边界
样本数量n比较大，特征向量m比较小
				- 建模

					- 参数初始化
					- 假设函数

						- logistice回归的假设函数，线性核函数，衡量了决策边界到样本点的距离

					- 参数学习

						- 代价函数与logistic函数的代价函数类似，但使用的不是sigmoid，而是两条直线线段

							- 这使得Z值需要处于 [-1,1] 之外
							- SVM拟合出的模型的决策边界，和样本值存在一定间距margin。
							- 数学解释
原因在于，代价函数优化过程中，会使theta范数最小，而数据向量X在theta上的投影要尽可能大，这使得theta向量和数据向量X基本同反向。而决策边界又与theta向量垂直。最终起到了决策边界和样本具有最大间距的效果。
							- 决策边界距离正负样本的最小距离称为间距，SVM可以得到间距最大的决策边界

						- 优化算法

				- 诊断
				- 调试

					- 相比正则化的logistics算法，正则化项的正则化参数被除去，同时在代价函数前增加参数C。优化方法有所变化

						- 当C参数巨大时，代价函数对异常点(outlier) 比较敏感，造成决策边界不大理想。C参数较小时就没有这个问题。

			- 带有核算法的SVM
高斯核函数 kernel

				- 原因/用法：用于拟合非常复杂的分类边界
样本数量n比较小，特征向量m比较大
				- 建模

					- 参数初始化

						- 归一化处理：特征缩放

					- 假设函数：核算法的概念

						- 核函数

							- 理解

								- 衡量原始输入特征和标记向量的距离

							- 数学特性

								- C值、sigmoid值对于核函数的数学特性的影响
								- C值的规律和正则化参数是反过来的
								- sigmoid值的规律，越大，图形越平缓，偏差和方差就越小
								- 依赖这种数学特性，能够有效衡量样本数据和标记点的接近程度

						- 原始输入特征向量与标记向量的相似度计算值，成为一个样本的新特征，一个样本和其他任一个标记向量都会产生一个新特征。

							- 理解

								- 衡量一个样本和其他样本的相似程度

					- 参数学习

						- 代价函数
						- 优化算法

				- 诊断
				- 调试

			- 带有其他类型核函数的SVM

				- 根据应用来选择
				- 使用核函数需要符合默塞尔定理

		- 以上的区别

	- 处理问题

		- 线性模型处理回归问题

			- 单变量来预测某个连续值

				- 建模-单变量线性回归算法

					- 代价函数 采用平方误差函数
					- 平方误差函数为凸函数，一定有全局最小值，因此采用梯度下降一定有全局最小值

						- 1/2m是为了让计算更方便

					- 批量梯度下降
batch gradient descent

						- 同步更新所有参数

				- 调试-

					- 过拟合出现

						- 正则化方法的理解角度

							- 梯度下降
							- 线性方程组解析解

			- 多变量来预测某个连续值

				- 建模-多变量线性回归算法

					- 尽量使特征值x_i处于[-1,1]/[-0.5,0.5]之间

					  特征值的数值范围在[-1,1]之间时，梯度下降速度更快一些。
					  特征值的数值范围如果差异大，梯度下降过程的振荡过多

						- 如果不同变量的变化范围差距较大，使用特征缩放-feature scaling

							- 统一除以range/ standard deviation

						- 均值归一化 mean normalization

							- 统一减去平均值

					- 代价函数仍为平方误差函数
					- 梯度下降公式

						- 可能找不到全局最优，只能找到局部最优
						- 调试 debugging
确保梯度下降成功收敛

							- 成功收敛，代价函数会持续下降，这要求有合适的学习率
							- 但迭代后代价函数变化值小于某一阈值时，可认为收敛结束。代价函数变化小，偏微分小，参数调节幅度小，梯度下降慢，趋近平稳

								- 有效阈值很难判断

							- 学习率 learning rate重要，过小，收敛过慢，过大，振荡

								- 一个比较有效的方法是每次扩大3倍

				- 建模-多项式线性回归算法

					- 构造特征变量，在仅有单一特征变量的情况下，构造出多项式方程做拟合

				- 调试

		- 线性模型处理分类问题

			- 增加决策层
使用预测函数

- 神经网络
//非线性分类器
//non-linear hypothesis

	- 特点：相比于线性模型
优点：即使原始输入特征维数极大，也能够有很好的效果
	- 分类-基础模型

		- 网络结构分类
network architecture

			- 单层：单隐层神经网络 = logistics回归

				- 逐层抽象数据的特征
举例：
非线性函数选用 sigmoid时，实现逻辑上NOT、OR、AND、XOR 、XNOR函数的功能

			- 多层：深层神经网络
人工神经网络（ANN，Artificial Neural Network）
（深度学习 deep learning）

				- 神经元

					- 偏置单元

				- 网络结构

					- 不是直接以原始输入特征作为输入，而是先对原始输入特征通过隐藏层进行一系列训练，计算出更复杂的特征，来作为最终“逻辑回归”的输入,从而得到一个复杂的非线性假设。

						- 输出层可看作logistics回归

					- 前馈网络
					- 反馈网络
					- 图网络

		- 前馈神经网络 FNN
类似多层的logistic回归

			- 二元分类问题的神经网络算法

				- 输出层的单元数为1个，值为[0，1]，其他和多元分类问题的神经网络算法基本一致。

			- 多元分类问题的神经网络算法
multi-class classification

				- 输出层的单元数 s_j表示分类的维度
1表示是，0表示不是
				- 建模

					- 假设函数

						- 前馈网络
						- 输入层、隐藏层、输出层

					- 参数学习

						- 随机初始化

							- 使用相同的初始参数，梯度下降过程中参数值的变化将一致，形成冗余和对称。随机化能够消除这种情况。

						- 学习准则
						- 代价函数

							- 交叉熵

								- 理解

									- 多层神经网络的代价函数，比较复杂
									- 正则化的代价函数

										- 正则化项：
将i个样本的L层中，每层j个单元的theta参数值全部加起来

							- 前向传播算法-（？FP）
逐层计算，最后得到代价函数‘

						- 优化算法

							- 反向传播算法 - BP算法

								- 理解

									- 求代价函数的偏微分，从而将误差error向前传播，计算每一层的误差error，进而调整每一层的theta
									- 上一层中某一特征值theta值远大，特征权重越大，代入sigmoid函数后值也越大，意味着对下一层特征变量的影响就越大
									- 由于误差方向传播是，前一层的误差等于 转置矩阵和后一层的乘积，再乘以sigmoid函数的导数。
									- 由于sigmoid函数的特性，当Z值逼近0时，斜率越大
误差反向传播时，对误差求偏微分得到的数值也越大
由于转置矩阵的运算特性，前向传播时的theta值越大，反向传播时计算用的theta值也越大

										- 为什么乘以转置矩阵，对应的theta项还对吗？？？

											- 需要推导理解一下

									- 相应地可以知道，前项传播时设置的theta值越大，以及Z值越接近0（此时激活值a = 0.5，意味着完全不能判断是还不是），反向传播时对算出的误差值越大，惩罚幅度越大，theta的调整幅度越大。

								- 编程 | 梯度检验

									- 用数值分析方法（双侧差分）计算偏微分，并与BP算法结果比较，校对两者是否接近
									- BP算法验证正确后，在编程中关闭梯度检验过程

				- 诊断
				- 调试

		- CNN
		- RNN

	- 分类-进阶模型

### 模型独立的学习方式

- 监督学习
- 无监督学习

	- 原因/用法：
自动寻找数据的结构关系
	- 聚类
clustering

	  比如市场分割、社交网络、调整计算机集群

		- 原因/用法：
对于具有隐藏结构-簇 的数据，使用聚类算法

			- 举例：市场分割、社交网络分析、组织计算机集群、天文星系构成
			- 也可以用来做数据压缩

		- 用什么模型？
		- k-means 算法

			- 特点：最为常用的聚类算法
优点：
缺点：聚类中心的数量无有效的确定方法
			- 建模

				- 参数初始化

					- 随机初始化，避免k-means陷入局部最优

						- 随机选择K个样本作为聚类中心，k-means算法
						- 聚类中心数K < 样本数
						- 仅对K值较小的情况有效

					- 多次随机初始化，找出最终代价函数最小的初始化实例

				- 假设函数

					- 聚类数量的选择主要凭经验

						- 自动选择算法：
肘部法则 elbow method

							- 坐标轴横轴为聚类数量，纵轴为优化后代价函数，
找到曲线的拐点

						- 后续目的，匹配聚类中心的实际意义

					- 不断调整聚类点位置来聚类。调整位置的方法在于取归类后样本点平均值。

						- 当某个聚类中心无分配点时，去除

				- 参数学习

					- 代价函数
					- 优化算法

			- 诊断
			- 调试

	- 降维
Dimensionality Reduction

		- 原因/用法：
动机1：数据压缩，减少运算量
动机2：数据可视化

			- 比如对训练集预处理，提取训练集特征，降维，减少运算量

		- 数据压缩-主成分分析算法 PCA
(Principal Components Analysis)

			- 特点：
优点：
缺点：

				- 1、虽然可以减少特征值，但是不能用来防止过拟合，而是采用正则化方法
2、由于PCA过程会丢失信息，所以不宜在初始阶段就使用，仅在特征变量过多时使用

			- 建模

				- 参数初始化

					- 数据要均值标准化，使每个特征的数值相近

				- 假设函数

					- 理解

						- PCA算法的优化目标就是:   
1、降维后同一维度的方差最大
2、不同维度之间的相关性为0
3、最小投影误差和

							- 降维取重要程度高的K个成分 
							- k的取值决定了原矩阵的方差保留程度，一般保留99%/95%

						- 信息量保存能力最大的基向量一定是样本矩阵X的协方差矩阵的特征向量，并且这个特征向量保存的信息量就是它对应的特征值的绝对值。这个推导过程就解释了为什么PCA算法要利用样本协方差的特征向量矩阵来降维。

					- 过程

						- 计算协方差矩阵
						- 协方差矩阵奇异值分解
						- 投影得到新样本点

				- 参数学习

					- 代价函数

						- 找出使假设函数最小的K值

					- 优化算法

			- 诊断
			- 调试

		- 压缩重现-主成分分析算法

			- 特点：
优点：
缺点
			- 建模

				- PCA的逆过程

			- 诊断
			- 调试

	- 异常检测
Anomaly Detection

		- 原因/用法：
寻找数据中的异常数据

			- 欺诈检测
			- 工业领域：飞机引擎异常检测
			- 数据中心的计算机

		- 异常检测和监督学习的区别

			- 当数据集中的正样本(y=1,异常样本)非常少（0-20），而负样本（y=0，正常样本）非常多时，我们一般使用异常检测算法；当数据集中存在大量正样本和负样
本时，我们一般使用监督学习的方法
			- 对于异常检测问题来说，一个样本出现异常的情况是多种多样的，而且在不断的变化，可能过一段时间就会出现一个新的异常，如果使用监督学习的方法，很难
从少量异常样本中，学习到各种异常的情况；而对于异常检测算法来说，它只需要在大量的负样本（正常样本）中进行训练，而不必花时间去学习异常样本，只要一个样
本和大多数正常样本的分布不一样，就可以判定为异常。

		- 基于距离的异常检测算法
		- 基于分布的异常检测算法

			- 分类

				- 高斯分布

					- 使用e判定异常值
					- 使用预测率和召回率来判断算法效果
					- 不能捕捉特征之间的相关性，除非构造新特征来捕捉相关联的两个特征，能适应大规模的特征

				- 多元高斯分布

					- 自动捕捉特征之间的相关性，找出单元高斯分布不能找到的特征间的关系
					- 样本数量必须大于特征数量

				- 异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：x=log(x+c)，其中c为非负常数；或者x=xc，c为0-1之间的一个分数，等方法。(编者注：在python中，通常用np.log1p()函数，log1p就是log(x+1)，可以避免出现负数结果，反向函数就是np.expm1())
				- 协方差矩阵

					- 协方差矩阵上的对角线

			- 基于高斯分布的异常检测算法举例

				- 数据预处理

					- 数据集划分

						- 数据集划分为训练集、验证集和测试集
将异常数据主要划分到验证集和测试集中

					- 特征映射

						- 非高斯分布的数据集映射为高斯分布

						  实际上不转换也行，但转换后效果更好

							- 常采用x = log(x + c)

				- 建模

					- 假设函数

						- 计算出训练集的概率分布
						- 为每个样本确定特征向量
						- 确定每个特征服从的高斯分布

					- 参数学习

						- 代价函数
						- 优化算法

					- 当新样本特征向量和模型的预测值偏离到一定程度时，判定为异常点

				- 诊断
				- 调试

		- 基于深度的异常检测算法
		- 基于划分的异常检测算法

	- 推荐系统
Recommender Systems

		- 基于内容的推荐系统

			- 无标签数据

		- 协同过滤

			- 有标签数据

## 机器学习系统设计

### 基本流程

- 快速实现简单模型

	- 建模（训练、验证、测试）使得模型能够很好地拟合数据

- 交叉验证和测试，进行误差分析，提取特征

	- 把模型的某些指标转化为实际意义（特征）

- 设计可量化的指标，用于评估模型的改进情况，帮助改进模型
- 评估加入新特征后模型的运行情况

### 误差分析/误差评估

- 不对称分类问题（偏斜类分类问题）的误差分析
skewed classfication
y = 1 出现的概率很小

	- 量化指标：
查准率 prediction
召回率 recall

		- 通过设置合理的分类临界值，
调整查准率和召回率的变化，服务实际情况
		- 设置评估度量值，也就是评估多个量化指标的综合指标
对于这种情况可以使用【F_1值】
F_1 = 2 AB/(A+B)
实际上是两者倒数的平均值的导数

## 机器学习编程

### python

- numpy
- scikit-learn
- scipy
- matplotlib

*XMind: ZEN - Trial Version*