---
typora-root-url: jpg\3 计算机系统
---

# Parallel, Concurrent and Distributed programming in Java

These are my solutions to these three courses. Below I added short comments to
each week so that I remember better what it's about and that you, the reader,
could have a better idea too.

该课程从任务的分解出发，介绍了并行执行任务的Java ForkJoin FrameWork和Java Stream技术，以一维的数组的stencil 为例，介绍了Barrier和Phaser等优化循环结构的技术，最终达到以流水线的方式执行一个大的任务的效果。

### 

## Parallel programming in Java

补充内容：[并行计算体系课程](http://www.icourses.cn/sCourse/course_3058.html)

### Week 0

Preparation week. The purpose is to test your environment. No actual task to do.

### Week 1 ForkJoin (**Task-level Parallelism**)

In this week we need to calculate reciprocal array sum. We're using Java's
ForkJoin framework to parallelize our calculations. There are different ways to
interact with the ForkJoin framework, and in this week we're extending
`RecursiveAction` and overriding `compute()` method. In `compute()` method we
determine current size of the current range of the that we need to process. If
it's small enough, we are processing it immediately. Otherwise, we divide it
into two parts (left and right) and process them recursively.

The minimal threshold value determines how many ForJoin tasks we will totally
create. Notice that I didn't manage to pass the tests locally and I didn't
submit the solution to the Coursera grader. Many students reported the same
problem on the forums. Still, there was a significant improvement over the
linear execution, just not as big as expected by the creators of the homework.

#### 1.1 Task Creation and Termination (Async,  Finish)

一个任务可以自然分解成多个结果互不依赖的子任务，让每个任务并行执行（用线程来执行），每个执行线程如果由不同的CPU进行，就是并发任务。

```
finish {
  async S1; // asynchronously compute sum of the lower half of the array
  S2;       // compute sum of the upper half of the array in parallel with S1
}
S3; // combine the two partial sums after both S1 and S2 have finished
```



#### 1.2 Tasks in Java's Fork/Join Framework 

Fork/Join框架是如何实现任务的分解和异步执行的。

[RecursiveTask和RecursiveAction的使用总结

> ## Class ForkJoinTask<V>
>
> - [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html)
> - - java.util.concurrent.ForkJoinTask<V>
>
> - - All Implemented Interfaces:
>
>     [Serializable](https://docs.oracle.com/javase/8/docs/api/java/io/Serializable.html), [Future](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html)<V>
>
>   - Direct Known Subclasses:
>
>     [CountedCompleter](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountedCompleter.html), [RecursiveAction](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/RecursiveAction.html), [RecursiveTask](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/RecursiveTask.html)

```java
private static class ASum extends RecursiveAction {
    int[] A; // input array
    int LO, HI; // subrange
    int SUM; // return value
    . . .
    @Override
    protected void compute() {
        //在该方法中要完成任务分解的逻辑
        
        subTaskA1.fork()
        subTaskA2.join() //实际上就是Future.get()方法，Future.get()在得到计算完成返回的结果前，会保持阻塞。
    } // compute()
}
```



```java
ForkJoinPool.submit(RecursiveTask taskA)
ForkJoinPool.invoke(RecursiveAction taskB1)
ForkJoinPool.invokeAll(RecursiveAction taskB2)
//在ForkJoinPool中，task被多个线程同时执行。task的分解逻辑由 compute()方法来实现，

```



#### 1.3 Computation Graphs, Work, Span, Ideal Parallelism 计算图、工作量、张量（关键路径）和并行性

任务执行之间具有偏序关系，只能串行执行时 [partially ordered set](http://en.wikipedia.org/wiki/Partially_ordered_set)，将会限制并行算法速度 [Analysis of parallel algorithms](https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms)

> For *fork*–*join* programs, it is useful to partition the edges into three cases:
> \1. *Continue* edges that capture sequencing of steps within a task.
> \2. *Fork* edges that connect a fork operation to the first step of child tasks.
> \3. *Join* edges that connect the last step of a task to all *join* operations on that task.

#### 1.4 Multiprocessor Scheduling, Parallel Speedup

多任务调度和加速比的概念

> We defined *T*_{P}*P* as the execution time of a CG on *P* processors, and observed that
>
> T(无限多核并行)≤ T(多核并行) ≤ T(单核，串行)

#### 1.5 Amdahl's Law

串行任务比例和加速比的概念

> This observation follows directly from a lower bound on parallel execution time that you are familiar with, namely *T*_P*P* *≥* *SPAN*(*G*). If fraction *q* of *WORK(G)* is sequential, it must be the case that *SPAN*(*G*) *≥* *q* *×* *WORK(G)*. Therefore, *Speedup(P)* = *T*_11*/T*_P*P* must be *≤* *WORK(G)**/*(*q* *×* *WORK(G)*) = 1*/q* since *T*_11 = *WORK(G)* for greedy schedulers.

### Week 2 Streams  (**Functional Parallelism**)

This week we're using Java Streams API to build a pipeline that computes some
student analytics. Nothing difficult here, you just need to understand which
data type you have at each pipeline stage and convert data types accordingly if
needed.

#### 2.1 Futures: Tasks with Return Values

[ForkJoinTask]( https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html)

RecursiveTask，带有返回值的任务，返回值可以放置在Future对象里

RecursiveAction，不带有返回值的任务。

#### 2.2 Futures in Java's Fork/Join Framework

[RecursiveTask和RecursiveAction的使用总结](https://www.cnblogs.com/jelly12345/p/12121455.html)

#### 2.3 Future 对象和记忆化技术 Memoization 

记忆化，记录有哪些Future已被标记为待计算的任务。The memoization pattern lends itself easily to parallelization using futures by modifying the memoized data structure to store {(*x*_11, *y*_11 = *future*(*f* (*x*_11))), (*x*_22, *y*_22 = *future*(*f* (*x*_22))), . . .}. The lookup operation can then be replaced by a *get()* operation on the future value, if a future has already been created for the result of a given input.

#### 2.4 用Java Streams 处理的带返回值的并行任务

和聚合运算agregation operation相关的任务，即可以对集合中的每个元素单独处理，又需要返回结果。这种任务非常适合并行运算，Java Stream提供了这种解决方案。

[Java 8 流的函数式编程理念](https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html)：[集合和流](https://docs.oracle.com/javase/tutorial/collections/streams/)、[Stream](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html)、[Collector](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html)

串行流 Stream() 和 并行流 parallelstream()或Stream.of(T []).parallel()

mapReduce 中间作业和终点作业

```
students.stream()
    .filter(s -> s.getStatus() == Student.ACTIVE)
    .mapToInt(a -> a.getAge())
    .average();
```

[当人们谈论函数式编程时，许多令人眼花缭乱的“函数式”特征都将涵盖其中。他们会提及不可变数据(1)、一等函数(2)、尾部调用优化(3)，这些都是有助函数式编程的语言层面的功能特性；他们还会提及映射（mapping）、化简（reducing）、管道（pipeling）、递归（recursing）、柯里化（currying）(4)、以及高阶函数的应用，这些均为编写函数式代码的技术和技巧；他们还会提及并行化（parallelization）(5)、惰性求值（lazy evaluation）(6)、确定性（determinism）(7)，这些正是函数式程序的优势所在。](https://docs.oracle.com/javase/tutorial/collections/streams/parallelism.html#executing_streams_in_parallel)

#### 2.5 Data Race and Determinism 数据竞争和确定性

A parallel program is said to be *functionally deterministic* if it always computes the same answer when given the same input, and *structurally deterministic* if it always computes the same computation graph, when given the same input. The presence of data races often leads to functional and/or structural nondeterminism because a parallel program with data races may exhibit different behaviors for the same input, depending on the relative scheduling and timing of memory accesses involved in a data race. In general, the absence of data races is not sufficient to guarantee determinism. However, all the parallel constructs introduced in this course (“Parallelism”) were carefully selected to ensure the following *Determinism Property*:

如果一个并行程序总是在输入相同的情况下计算相同的结果，那么它就被称为功能确定的;如果它总是在输入相同的情况下计算相同的计算图形，那么它就被称为结构确定的。数据竞争的存在通常会导致功能和/或结构上的不确定性，因为具有数据竞争的并行程序对于相同的输入可能会表现出不同的行为，这取决于数据竞争中涉及的内存访问的相对调度和时间。

实际编程中要避免出现这样的问题

### Week 3 PCDP  (Loop Parallelism) 

This week we are learning how to use PCDP library (a library developed to teach
parallel computations) to parallelize matrix multiplications. Again, the task is
trivial as you only need to replace one method name with another.

对于一个可以分解为多个子任务的任务，很可能要执行多次。这个时候一般采用的是loop循环来执行。在loop循环结构中，每次循环之间是设计为同步还是异步执行，如何优化循环结构下的并行运算，是一个值得探讨的问题。

#### 3.1 Parallel Loops

以图像处理为例，将图像看作一个matrix，每行的像素可以并行处理，处理的线程是异步执行的。

```java
//条件1：任务具有异步执行的特征
finish {
for (p = head; p != null ; p = p.next) async compute(p);
}
//条件2：任务的数量和规模是已知的，比如下方
forall (i : [0:n-1]) a[i] = b[i] + c[i]
    
//适用
a = IntStream.rangeClosed(0, N-1).parallel().toArray(i -> b[i] + c[i]);
```

#### 3.2 Parallel Matrix Multiplication

[线性代数中的矩阵相乘问题，矩阵快速幂算法](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)，可以并行计算更块地解决。

```java
for(i : [0:n-1]) {
  for(j : [0:n-1]) { c[i][j] = 0;
    for(k : [0:n-1]) {
      c[i][j] = c[i][j] + a[i][k]*b[k][j]
    }
  }
}
```

#### 3.3 Barriers in Parallel Loops

```java
forall (i : [0:n-1]) {
        myId = lookup(i); // convert int to a string 
        print HELLO, myId;
        BARRIER//关键代码，所有线程执行到这一行后，都暂时阻塞，直到全部线程都抵达。java中用countDownLatch或者其他实现方法
        print BYE, myId;
}
```

如果上面的loop代码，采用并行执行，多个线程同时打印，每个结果之间是无序输出的。需要一种机制能够让线程之间互相等待。

> Java 8中的实现
>
> CountDownLatch	
> A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.
>
> CyclicBarrier	
> A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.

#### 3.4 One-Dimensional Iterative Averaging

[Stencil codes](https://en.wikipedia.org/wiki/Stencil_code)

```java
//串行，共n steps
for (iter: [0:nsteps-1]) {
    //并行 n-1 tasks created，
  forall (i: [1:n-1]) {
    newX[i] = (oldX[i-1] + oldX[i+1]) / 2;
  }
  swap pointers newX and oldX;
}
//创建(n-1)*(n-1) task
```

```java
//并行 n-1 tasks created
forall ( i: [1:n-1]) {
  localNewX = newX; localOldX = oldX;//获取一个引用
    //串行 共 n steps
  for (iter: [0:nsteps-1]) {
    localNewX[i] = (localOldX[i-1] + localOldX[i+1]) / 2;
    NEXT; // Barrier 每一个线程执行到这一步的时候，都暂停
    swap pointers localNewX and localOldX;
  }
}
//创建(n-1)次task + 执行(n-1)*(n-1)次NEXT
```

优点：This is a significant improvement since creating tasks is usually more expensive than performing barrier operations.

#### 3.5  Iteration Grouping/Chunking in Parallel Loops

```java
//创建 n-1 个任务，n >> NumberOfCoreOfComputer 时，非常不利也无必要
forall (i : [0:n-1]) a[i] = b[i] + c[i]

//改进思路：对任务划分分组
forall (g:[0:ng-1])
  for (i : mygroup(g, ng, [0:n-1])) a[i] = b[i] + c[i]
  //分组方式1，分块，chunck 比如[1,10] [11,20] [21,30]
  //分组方式2，循环，cyclic 比如[1,4,7,10] [2,5,8,11] [3,6,9,12]
  //具体可以使用PCDP的方法
```

For convenience, the PCDP library provides helper methods, *forallChunked()* and *forall2dChunked()*, that automatically create one-dimensional or two-dimensional parallel loops, and also perform a block-style iteration grouping/chunking on those parallel loops. An example of using the *forall2dChunked()* API for a two-dimensional parallel loop (as in the matrix multiply example) can be seen in the following Java code sketch:

### Week 4 Fuzzy phasers （**Dataflow Synchronization and Pipelining**）

In this week we explore the functionality of fuzzy phasers. The idea is that if
we have an array which average sum we need to compute, we can split work into
`N` tasks and do it concurrently. The improvement is that a task `i` only has to
for for tasks `i-1` and `i+1` to complete. To achieve that, we use `N` phasers
and we call their methods `arrive()` and `awaitAdvance(int phase)`. Notice here
since we run multiple iterations over the same array, this `phase` number keeps
growing and it's crucially important to keep track of the current phase.

``` text
    1 2 3
     \ /
    1 2 3
```

This illustrates that task 2 depends on completion of tasks 1 and 3.

#### 4.1 Split-phase Barriers with Java Phasers

```java
// initialize phaser ph	for use by n tasks ("parties") 
Phaser ph = new Phaser(n);
// Create forall loop with n iterations that operate on ph 
forall (i : [0:n-1]) {
  print HELLO, i;
  int phase = ph.arrive(); //保证所有并行线程都打印 hello后，才会越过barrier
  
  myId = lookup(i); // convert int to a string

  ph.awaitAdvance(phase);//多个并行的线程阻塞在这个位置
  print BYE, myId;
}
```

#### 4.2 Point-to-Point Sychronization with Phasers

![javaPhaser并行优化](/javaPhaser并行优化.png)

#### *4.3 One-Dimensional Iterative Averaging with Phasers

```java
// Allocate array of phasers
Phaser[] ph = new Phaser[n+2]; //array of phasers
for (int i = 0; i < ph.length; i++) ph[i] = new Phaser(1);

// Main computation 
forall ( i: [1:n-1]) {
  for (iter: [0:nsteps-1]) {
    newX[i] = (oldX[i-1] + oldX[i+1]) / 2;
    ph[i].arrive();
    //效果：forall产生的多个并行线程，统一步调，全部算完第一轮后，再进入第二轮
    if (index > 1) ph[i-1].awaitAdvance(iter);
    if (index < n-1) ph[i + 1].awaitAdvance(iter); 
    swap pointers newX and oldX;
  }
}
```

```java
// Allocate array of phasers proportional to number of chunked tasks 
Phaser[] ph = new Phaser[tasks+2]; //array of phasers
for (int i = 0; i < ph.length; i++) ph[i] = new Phaser(1);

// Main computation 
forall ( i: [0:tasks-1]) {
  for (iter: [0:nsteps-1]) {
    //每个线程负责的chunck的边界是有关联的，只要先把边界搞定，剩下的块内部的部分可以异步计算
    // Compute leftmost boundary element for group
    int left = i * (n / tasks) + 1;
    myNew[left] = (myVal[left - 1] + myVal[left + 1]) / 2.0;
    
    // Compute rightmost boundary element for group 
    int right = (i + 1) * (n / tasks);
    myNew[right] = (myVal[right - 1] + myVal[right + 1]) / 2.0;
    
    // Signal arrival on phaser ph AND LEFT AND RIGHT ELEMENTS ARE AV 
    // 此线程标记第 index 次同步中自己负责的部分的边界已经计算好了
    int	index = i + 1;
    ph[index].arrive();
    
    // Compute interior elements in parallel with barrier 
    // 异步执行自己负责的chunck
    for (int j = left + 1; j <= right - 1; j++)
      myNew[j] = (myVal[j - 1] + myVal[j + 1]) / 2.0;
    // Wait for previous phase to complete before advancing 
    if (index > 1) ph[index - 1].awaitAdvance(iter);
    if (index < tasks) ph[index + 1].awaitAdvance(iter);
    swap pointers newX and oldX;
  }
}
```

#### 4.4 Pipeline Parallelism

```java
// Code for pipeline stage i
while ( there is an input to be processed ) {
  // wait for previous stage, if any 
  if (i > 0) ph[i - 1].awaitAdvance(); 
  
  process input;
  
  // signal next stage
  ph[i].arrive();
}
```

#### 4.5 **Data Flow Parallelism**

```java
async( () -> {/* Task A */; A.put(); } ); // Complete task and trigger event A
async( () -> {/* Task B */; B.put(); } ); // Complete task and trigger event B
asyncAwait(A, () -> {/* Task C */} );	    // Only execute task after event A is triggered 
asyncAwait(A, B, () -> {/* Task D */} );	  // Only execute task after events A, B are triggered 
asyncAwait(B, () -> {/* Task E */} );	    // Only execute task after event B is triggered

//等价于
asyncAwait(A, () -> {/* Task C */} );	    // Only execute task after event A is triggered 
asyncAwait(A, B, () -> {/* Task D */} );	  // Only execute task after events A, B are triggered 
asyncAwait(B, () -> {/* Task E */} );    	// Only execute task after event B is triggered 
async( () -> {/* Task A */; A.put(); } ); // Complete task and trigger event A
async( () -> {/* Task B */; B.put(); } ); // Complete task and trigger event B
```



## Concurrent programming in Java

[线程池大小设计](http://ifeve.com/how-to-calculate-threadpool-size/)

### Week 1 Locks 线程和线程同步问题

In this week we were introduced to locks, the basic primitive for
synchronization. Java's `synchronized` keyword is pretty much the same lock that
guards the whole method. Lock, however, can be made more granular. In this week
we applied `ReadWrite` lock to benefit from the fact that read only methods can
have concurrent access to the data, while read-and-write methods should be
guarded by a lock.

并发编程目标：1)分时计算2)提高运算速度

#### 1.1 Java Threads

Thread.join() 可能引起死锁问题。

多线程情况下的竞态条件race condition

#### 1.2 Structured Locks

synchronized同步机制，又称为structured lock，因为看起来很结构化，很规整

Java提供的Synchronized， 用来解决race condition问题的一种方法

Java提供的thread.wait() thread.notify(），解决生产者消费者问题

#### 1.3 Unstructured Locks

[AQS同步器框架](https://blog.csdn.net/fxkcsdn/article/details/82217760)

因为Synchonized不能解决链表结构下hand-over-hand lock的需要[具体理由](https://www.cnblogs.com/huangzifu/p/7680196.html)，又提供了ReentrantLock-Condition，更加灵活

进一步地，基于ReentrantLock，Java 提供 Read-Write-Lock功能，对于同一对象，多个线程共同读写。

unstructured lock还支持trylock，基于trylock，可以实现先尝试获取锁，获取不到可继续执行其他代码。而synchronized只能实现为先尝试获取锁，获取不到就阻塞 block！

#### 1.4 Liveness and Progress Guarantees

《Java并发编程实战》第 12 章中说到：

> 并发测试大致分为两类，即安全性测试与活跃性测试。在第一章，我们将安全性定义为“不发生任何错误的行为”，而将活跃性定义为“某个良好的行为终究会发生”。

线程活跃度（progress guarantee）问题的几个原因

1. 死锁 典型代码（4个条件）
2. 活锁 典型代码（2个while，1个共享变量）
3. 饥饿（线程优先级过低。线程调度时分配不到时间片）

The term “liveness” refers to a progress guarantee. The three progress guarantees that correspond to the absence of the conditions listed above are *deadlock freedom*, *livelock freedom*, and *starvation freedom*.

#### 1.5 Dining Philosophers Problem 哲学家就餐问题

一种典型的线程并发模型

使用sychronized解决可能产生死锁（所有哲学家线程同时拿起左叉子或者右叉子，所有线程阻塞），进一步修改可解决[编程模拟](https://leetcode-cn.com/problems/the-dining-philosophers/solution/1ge-semaphore-1ge-reentrantlockshu-zu-by-gfu/)

使用reentrantLock解决可能产生活锁（所有哲学家同时拿起左叉子或者右差值，虽然没有阻塞，但会再次loop）



### Week 2 Isolation 解决同步问题：隔离

This week we are introduced to the concepts of critical sections, atomic
variables and isolation. Being a more high-level synchronization construct,
isolation allows for simpler semantics providing the same or even better level
of efficiency. To make things faster in the homework, we use PCDP library and
object-level isolation, not global isolation.

#### 2.1 Critical Sections

并发程序中的临界区，实现方法有悲观并发策略和乐观并发策略，都是对整个代码块加锁。可以用sychronized（管程技术==临界区+条件变量）或者reentrantLock（）来构造出临界区。

#### 2.2 Object-Based Isolation

对于双向链表一类的对象，如果有多个线程在同一个双向链表上进行write写操作，那么为了增加并发性，肯定要需要一种对每个节点都可以单独设锁的机制monitor。

或者类似银行转账问题，所有账户组成一个数组，多个线程在数组元素之间执行转账操作（src账户-account，dst账户+account），那么比较好的同步方法是减小锁粒度，只锁住单个数组元素而不是整个数组

对象隔离，方法一：悲观锁

monitor 监视器锁，在Java中也实现一并实现在sychronized关键字中

sychronized是修饰方法时，会等于隔离/锁住一个对象。

```
isolated(cur, cur.prev, cur.next, () -> {
    . . . // Body of object-based isolated construct
});
```



#### 2.3 Spanning Tree Example

乐观锁的基石，汇编指令集的compareAndSet()方法

场景：最小生成树算法中，需要在BFS/DFS过程中将某一节点设置为父节点，此步骤下只针对一个对象，采用的是compareAndSet，compare 新节点未被连接到生成树，set 新节点

#### 2.4 Atomic Variables

对象隔离，方法二：乐观锁

基于CAS的乐观锁的典型应用——原子变量，以及原子引用Atomic reference（compareAndSet)

原子变量中常见的两种pattern

```
j=cur.getAndAdd(1) //原子变量java代码

等价于 以下原语
isolated (cur){ 
	j=cur;cur=cur+1;
} 
```

<img src="/并发处理一个对象数组.png" alt="并发处理一个对象数组"  />

#### 2.5 Read, Write Isolation

基于读写锁实现的对象隔离。——并发容器类。比如Map，LinkedList

**ReadWriteLock同Lock一样也是一个接口，提供了readLock和writeLock两种锁的操作机制，一个是只读的锁，一个是写锁。**假设你的程序中涉及到对一些共享资源的读和写操作，且写操作没有读操作那么频繁。

### Week 3 Actors

This week we are introduced to the concept of actors. To explain it in my own
words, actors are independent units of computation that you can communicate with
using `send` method. In the miniproject we need to implement the famous Sieve of
Eratosthenes using actors. To do that, we create a chain of actors that
one-by-one check whether a given number is divisible by the range of numbers,
assigned to the given actor. To make it more efficient, a single actor is
assigned a bunch of numbers, e.g. 1000, not just one.

#### 3.1 Actor Model 多线程编程的一种新的异步模型

##### 对象隔离的局限性和Actor模型的提出

> 两个线程同时尝试购买最后一件商品时，如果没有锁就可能出现多个线程同时断定计数器的值大于或等于购买数量，然后错误地递减计数器，从而导致出现负数。
>
> 然而，问题的根源在于一个请求对应一个线程。
>
> 另外，在高度竞争的阶段，很有可能出现很长的线程队列，他们都在等待递减计数器。但使用队列的方式的问题在于可能造成众多阻塞线程，也就是每个线程都在等待轮到它们去执行一个序列化的操作。
>
> 所以，应用设计者一不小心，内在的复杂性就有可能将多核多线程的应用变成单线程的应用，或者导致工作线程之间存在高度竞争。或者也有可能因为一处并发编程的失误操作导致全局的错误（某一处代码忘记采用同步方法）
>
> Actor模型优雅的解决了这个难题，为真正多线程的应用提供了一个基础支持。
>
> 一般而言，有两种策略用来在并发进程/线程中进行通信：共享数据、消息传递。而实现消息传递有两种常见类型：基于`channel`的消息传递、基于`Actor`的消息传递。（回想一下操作系统中消息传递的方法：基于内核的消息队列、管道和基于文件系统的共享内存）
>
> Actor模型为并发而生，是为解决高并发的一种编程思路。使用并发编程时需要特别关注锁与内存原子性等一系列的线程问题，Actor模型内部的状态由自身维护，也就是说Actor内部数据只能由它自己通过消息传递来进行状态修改，所以使用Actor模型可以很好地避免这些问题。
>
> 作者：JunChow520
> 链接：https://www.jianshu.com/p/d803e2a7de8e
> 来源：简书

##### Actor模型

临界区、锁和基于锁的各种同步组件和容器，全是从共享内存的通信机制出发，实现进程/线程间的同步。Actor是基于消息传递的通信机制，实现进程/线程间数据的交换，也可以看作一种同步。

Actor是由状态（local state）、行为（behavior）、邮箱（mailbox）三者组成的。

- 状态（local state）：状态是指actor对象的变量信息，状态由actor自身管理，避免并发环境下的锁和内存原子性等问题。
- 行为（behavior）：行为指定的是actor中计算逻辑，通过actor接收到的消息来改变actor的状态。
- 邮箱（mailbox）：邮箱是actor之间的通信桥梁，邮箱内部通过FIFO消息队列来存储发送发消息，而接收方则从邮箱中获取消息。

[传统多数流行的语言并发是基于多线程之间的共享内存，使用同步方法防止写争夺，Actors使用消息模型，每个Actor在同一时间处理最多一个消息，可以发送消息给其他Actor，保证了单独写原则。从而巧妙避免了多线程写争夺。和共享数据方式相比，消息传递机制最大的优点就是不会产生数据竞争状态。实现消息传递有两种常见的类型：基于channel（golang为典型代表）的消息传递和基于Actor（erlang为代表）的消息传递。](https://studygolang.com/articles/23938?fr=sidebar)

[并发机制:CSP vs Actor模型以及Golang实现](https://blog.csdn.net/qq_32702033/article/details/104415434)

#### 3.2 Sieve of Eratosthenes 爱拉托逊斯筛法，Actor并发编程模型的典型应用

**将多个Actor合并成一个流水线运作的模式**！！！

```
public void process(final Object msg) {
  int candidate = (Integer) msg;
  // Check if the candidate is a non-multiple of the "local prime".
  // For example, localPrime = 2 in the Non-Mul-2 actor
  boolean nonMul = ((candidate % localPrime) != 0);
  // nothing needs to be done if nonMul = false
  if (nonMul) {
    if (nextActor == null) { 
      . . . // create & start new actor with candidate as its local prime
    }
    else nextActor.send(msg); // forward message to next actor
  } 
} // process
```

#### 3.4 Producer-Consumer Problem with Unbounded Buffer 

体会以下阻塞同步的生产者消费者模型和消息传递异步的生产者消费者模型

```
//consumer thread
while(buffer empty){}
process item removed from buffer

//consumer thread
process(s){
    process item s 
    buffer.send("ready")
}
```

#### 3.5 Producer-Consumer Problem with Bounded Buffer 

消息拉取模型，可以用Semaphore实现。但不是消息传递式的。

![生产者消费者的消息拉取模型](/生产者消费者的消息拉取模型.png)

如果要用Java实现为Actor模式，[使用Kilim](https://www.infoq.cn/article/2008/07/kilim-message-passing-in-java/)

如果用Golang实现

```go
import (
    "fmt"
    "time"
)

func consumer(cname string, ch chan int) {
    for i := range ch {
        fmt.Println("consumer--", cname, ":", i)
    }
    fmt.Println("ch closed.")
}

func producer(pname string, ch chan int) {
    for i := 0; i < 4; i++ {

        fmt.Println("producer--", pname, ":", i)
        ch <- i
    }
}

func main() {
    //用channel来传递"产品", 不再需要自己去加锁维护一个全局的阻塞队列
    data := make(chan int)
    go producer("生产者1", data)
    go producer("生产者2", data)
    go consumer("消费者1", data)
    go consumer("消费者2", data)

    time.Sleep(10 * time.Second)
    close(data)
    time.Sleep(10 * time.Second)
}
//可以看出，用golang实现生产者消费者非常简单，PV操作不需要各种加锁解锁，奥妙就在于CSP模型，即golang提倡的用通信代替共享内存。
```



### Week 4  Concurrent Data Structures 并发容器

This week we are introduced to concurrent data structures. In the miniproject we
need to implement Boruvka's algorithm that builds Minimum Weighted Spanning Tree
(MST). The implementation should use granular locks (one per node) to secure
access from multiple threads.

#### 4.1 Optimistic Concurrency 乐观并发

The method call *A.compareAndSet(curVal, newVal)* invoked on *AtomicInteger A* checks that the value in *A* still equals *c**urVal*, and, if so, updates *A*’s value to *newVal* before returning *true*; 

详细解释乐观锁在Atomic Integer的应用，getAndAdd() == whlle loop + compareAndSet()

#### 4.2 Concurrent Queues 并发队列

#### 4.3 Linearizability 并发容器的方法涉及的语义：线性一致性

多个线程占用时间片，可以抽象成互不重叠的区间关系

[线性一致性，是分布式系统一致性的概念。](https://zhuanlan.zhihu.com/p/60641050)	

> 《多处理器编程的艺术》第三章指出：并发对象的行为能够用它们的正确性和演进性有效的进行描述，并发对象的正确性是基于某种与顺序行为等价的概念，即三种正确性条件：静态一致性，顺序一致性和可线性化性（Linearizability），其约束的强度依次增大，可线性化是最强的约束，适用于描述由可线性化组件构成的高层系统。

![线性一致性](/线性一致性.png)

其实可以理解成Java中的happen before，尤其是单线程条件下的happen-before原则

> 单线程happen-before原则：在同一个线程中，书写在前面的操作happen-before后面的操作。
>
> 锁的happen-before原则：同一个锁的unlock操作happen-before此锁的lock操作。
>
> volatile的happen-before原则：对一个volatile变量的写操作happen-before对此变量的任意操作(当然也包括写操作了)。
>
> happen-before的传递性原则：如果A操作 happen-before B操作，B操作happen-before C操作，那么A操作happen-before C操作。
>
> 线程启动的happen-before原则：同一个线程的start方法happen-before此线程的其它方法。
>
> 线程中断的happen-before原则：对线程interrupt方法的调用happen-before被中断线程的检测到中断发送的代码。
>
> 线程终结的happen-before原则：线程中的所有操作都happen-before线程的终止检测。
>
> 对象创建的happen-before原则：一个对象的初始化完成先于他的finalize方法调用。
>
> 
>
> 作者：aworker
> 链接：https://www.jianshu.com/p/1508eedba54d
> 来源：简书
> 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

具体可以看分布式系统中全局快照的相关概念

#### 4.4 Concurrent HashMap 

The motivation for using *putIfAbsent()* is to ensure that only one instance of *key* is inserted in *chm*, even if multiple threads attempt to insert the same key in parallel. 

对于并发容器，有些操作是有线性一致性的，比如put(k, v)，比如putIfAbsent(k,v )，比如get(key)。也有一些不满足线性一致性，比如clear()，比如putall（）

#### 4.5 Minimum Spanning Tree Example 

如何在并发条件下执行最小生成树算法？ 2.3节

## Distributed programming in Java

### Week 1 Distributed Map Reduce

In this week we need to calculate PageRank using Spark tools to split data and
process it by chunks, a more advanced Map-Reduce model.

#### 1.1 Introduction to MapReduce

#### 1.2 Apache Hadoop Project 

#### 1.3 Apache Spark Framework

#### 1.4 TF-IDF Example 

#### 1.5 PageRank Example



### Week 2 Client-server programming

In this week we are introduced to the concept of client-server networking,
specifically to sockets. We need to implement a simple file server that serves
file over a socket by HTTP protocol.

#### 2.1 Introduction to Sockets

#### 2.2 Serialization and Deserialization

#### 2.3 Remote Method Invocation

#### 2.4 Multicast Sockets

#### 2.5 Publish-Subscribe Pattern

### Week 3 Message passing

This week we are using MPI to distribute matrix calculation. We send the whole
two matrices to all available instances and on each instance we calculate only
those rows that correspond to the current instance. When we're done, we send
results back and collect the resulting matrix.

#### 3.1 Single Program Multiple Data (SPMD) Model

#### 3.2 Point-to-Point Communication

#### 3.3 Message Ordering and Deadlock

#### 3.4 Non-Blocking Communications

#### 3.5 Collective Communication

### Week 4 Combining distribution and multi-threading

This week is basically the same as week 2 but now we need to execute each
request in a separate thread.

#### 4.1 Combining Distribution and Multithreading

#### 4.2 Multithreaded Servers

#### 4.3 MPI and Multithreading

#### 4.4 Distributed Actors

#### 4.5 Distributed Reactive Programming

## Author

2017-2018, Yuri Bochkarev
