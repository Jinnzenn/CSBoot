---
typora-root-url: jpg\3 计算机系统
---

# Parallel, Concurrent and Distributed programming in Java

These are my solutions to these three courses. Below I added short comments to
each week so that I remember better what it's about and that you, the reader,
could have a better idea too.

### 

## Parallel programming in Java

补充内容：[并行计算体系课程](http://www.icourses.cn/sCourse/course_3058.html)

### Week 0

Preparation week. The purpose is to test your environment. No actual task to do.

### Week 1 ForkJoin (**Task-level Parallelism**)

In this week we need to calculate reciprocal array sum. We're using Java's
ForkJoin framework to parallelize our calculations. There are different ways to
interact with the ForkJoin framework, and in this week we're extending
`RecursiveAction` and overriding `compute()` method. In `compute()` method we
determine current size of the current range of the that we need to process. If
it's small enough, we are processing it immediately. Otherwise, we divide it
into two parts (left and right) and process them recursively.

The minimal threshold value determines how many ForJoin tasks we will totally
create. Notice that I didn't manage to pass the tests locally and I didn't
submit the solution to the Coursera grader. Many students reported the same
problem on the forums. Still, there was a significant improvement over the
linear execution, just not as big as expected by the creators of the homework.

#### 1.1 Task Creation and Termination (Async,  Finish)

一个任务可以自然分解成多个结果互不依赖的子任务，让每个任务并行执行（用线程来执行），每个执行线程如果由不同的CPU进行，就是并发任务。

```
finish {
  async S1; // asynchronously compute sum of the lower half of the array
  S2;       // compute sum of the upper half of the array in parallel with S1
}
S3; // combine the two partial sums after both S1 and S2 have finished
```



#### 1.2 Tasks in Java's Fork/Join Framework 

Fork/Join框架是如何实现任务的分解和异步执行的。

[RecursiveTask和RecursiveAction的使用总结

> ## Class ForkJoinTask<V>
>
> - [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html)
> - - java.util.concurrent.ForkJoinTask<V>
>
> - - All Implemented Interfaces:
>
>     [Serializable](https://docs.oracle.com/javase/8/docs/api/java/io/Serializable.html), [Future](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/Future.html)<V>
>
>   - Direct Known Subclasses:
>
>     [CountedCompleter](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CountedCompleter.html), [RecursiveAction](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/RecursiveAction.html), [RecursiveTask](https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/RecursiveTask.html)

```java
private static class ASum extends RecursiveAction {
    int[] A; // input array
    int LO, HI; // subrange
    int SUM; // return value
    . . .
    @Override
    protected void compute() {
        //在该方法中要完成任务分解的逻辑
        
        subTaskA1.fork()
        subTaskA2.join() //实际上就是Future.get()方法，Future.get()在得到计算完成返回的结果前，会保持阻塞。
    } // compute()
}
```



```java
ForkJoinPool.submit(RecursiveTask taskA)
ForkJoinPool.invoke(RecursiveAction taskB1)
ForkJoinPool.invokeAll(RecursiveAction taskB2)
//在ForkJoinPool中，task被多个线程同时执行。task的分解逻辑由 compute()方法来实现，

```



#### 1.3 Computation Graphs, Work, Span, Ideal Parallelism 计算图、工作量、张量和并行性

任务执行之间具有偏序关系，只能串行执行时 [partially ordered set](http://en.wikipedia.org/wiki/Partially_ordered_set)，将会限制并行算法速度 [Analysis of parallel algorithms](https://en.wikipedia.org/wiki/Analysis_of_parallel_algorithms)

> For *fork*–*join* programs, it is useful to partition the edges into three cases:
> \1. *Continue* edges that capture sequencing of steps within a task.
> \2. *Fork* edges that connect a fork operation to the first step of child tasks.
> \3. *Join* edges that connect the last step of a task to all *join* operations on that task.

#### 1.4 Multiprocessor Scheduling, Parallel Speedup

多任务调度和加速比的概念

> We defined *T*_{P}*P* as the execution time of a CG on *P* processors, and observed that
>
> T(无限多核并行)≤ T(多核并行) ≤ T(单核，串行)

#### 1.5 Amdahl's Law

串行任务比例和加速比的概念

> This observation follows directly from a lower bound on parallel execution time that you are familiar with, namely *T*_P*P* *≥* *SPAN*(*G*). If fraction *q* of *WORK(G)* is sequential, it must be the case that *SPAN*(*G*) *≥* *q* *×* *WORK(G)*. Therefore, *Speedup(P)* = *T*_11*/T*_P*P* must be *≤* *WORK(G)**/*(*q* *×* *WORK(G)*) = 1*/q* since *T*_11 = *WORK(G)* for greedy schedulers.

### Week 2 Streams  (**Functional Parallelism**)

This week we're using Java Streams API to build a pipeline that computes some
student analytics. Nothing difficult here, you just need to understand which
data type you have at each pipeline stage and convert data types accordingly if
needed.

#### 2.1 Futures: Tasks with Return Values

[ForkJoinTask]( https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html)

RecursiveTask，带有返回值的任务，返回值可以放置在Future对象里

RecursiveAction，不带有返回值的任务。

#### 2.2 Futures in Java's Fork/Join Framework

[RecursiveTask和RecursiveAction的使用总结](https://www.cnblogs.com/jelly12345/p/12121455.html)

#### 2.3 Future 对象和记忆化技术 Memoization 

记忆化，记录有哪些Future已被标记为待计算的任务。The memoization pattern lends itself easily to parallelization using futures by modifying the memoized data structure to store {(*x*_11, *y*_11 = *future*(*f* (*x*_11))), (*x*_22, *y*_22 = *future*(*f* (*x*_22))), . . .}. The lookup operation can then be replaced by a *get()* operation on the future value, if a future has already been created for the result of a given input.

#### 2.4 用Java Streams 处理的带返回值的并行任务

和聚合运算agregation operation相关的任务，即可以对集合中的每个元素单独处理，又需要返回结果。这种任务非常适合并行运算，Java Stream提供了这种解决方案。

[Java 8 流的函数式编程理念](https://www.oracle.com/technical-resources/articles/java/ma14-java-se-8-streams.html)：[集合和流](https://docs.oracle.com/javase/tutorial/collections/streams/)、[Stream](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html)、[Collector](https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html)

串行流 Stream() 和 并行流 parallelstream()或Stream.of(T []).parallel()

mapReduce 中间作业和终点作业

```
students.stream()
    .filter(s -> s.getStatus() == Student.ACTIVE)
    .mapToInt(a -> a.getAge())
    .average();
```



#### 2.5 Data Race and Determinism 数据竞争和确定性

A parallel program is said to be *functionally deterministic* if it always computes the same answer when given the same input, and *structurally deterministic* if it always computes the same computation graph, when given the same input. The presence of data races often leads to functional and/or structural nondeterminism because a parallel program with data races may exhibit different behaviors for the same input, depending on the relative scheduling and timing of memory accesses involved in a data race. In general, the absence of data races is not sufficient to guarantee determinism. However, all the parallel constructs introduced in this course (“Parallelism”) were carefully selected to ensure the following *Determinism Property*:

如果一个并行程序总是在输入相同的情况下计算相同的结果，那么它就被称为功能确定的;如果它总是在输入相同的情况下计算相同的计算图形，那么它就被称为结构确定的。数据竞争的存在通常会导致功能和/或结构上的不确定性，因为具有数据竞争的并行程序对于相同的输入可能会表现出不同的行为，这取决于数据竞争中涉及的内存访问的相对调度和时间。

[当人们谈论函数式编程时，许多令人眼花缭乱的“函数式”特征都将涵盖其中。他们会提及不可变数据(1)、一等函数(2)、尾部调用优化(3)，这些都是有助函数式编程的语言层面的功能特性；他们还会提及映射（mapping）、化简（reducing）、管道（pipeling）、递归（recursing）、柯里化（currying）(4)、以及高阶函数的应用，这些均为编写函数式代码的技术和技巧；他们还会提及并行化（parallelization）(5)、惰性求值（lazy evaluation）(6)、确定性（determinism）(7)，这些正是函数式程序的优势所在。](https://docs.oracle.com/javase/tutorial/collections/streams/parallelism.html#executing_streams_in_parallel)

### Week 3 PCDP  (Loop Parallelism)

This week we are learning how to use PCDP library (a library developed to teach
parallel computations) to parallelize matrix multiplications. Again, the task is
trivial as you only need to replace one method name with another.

#### 3.1 Parallel Loops

以图像处理为例，将图像看作一个matrix，每行的像素可以并行处理，处理的线程是异步执行的。

```java
//条件1：任务具有异步执行的特征
finish {
for (p = head; p != null ; p = p.next) async compute(p);
}
//条件2：任务的数量和规模是已知的，比如下方
forall (i : [0:n-1]) a[i] = b[i] + c[i]
    
//适用
a = IntStream.rangeClosed(0, N-1).parallel().toArray(i -> b[i] + c[i]);
```

#### 3.2 Parallel Matrix Multiplication

[线性代数中的矩阵相乘问题，矩阵快速幂算法](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm)，可以并行计算更块地解决。

```java
for(i : [0:n-1]) {
  for(j : [0:n-1]) { c[i][j] = 0;
    for(k : [0:n-1]) {
      c[i][j] = c[i][j] + a[i][k]*b[k][j]
    }
  }
}
```

#### 3.3 Barriers in Parallel Loops

```java
forall (i : [0:n-1]) {
        myId = lookup(i); // convert int to a string 
        print HELLO, myId;
        BARRIER//关键代码，所有线程执行到这一行后，都暂时阻塞，直到全部线程都抵达。java中用countDownLatch或者其他实现方法
        print BYE, myId;
}
```

如果上面的loop代码，采用并行执行，多个线程同时打印，每个结果之间是无序输出的。需要一种机制能够让线程之间互相等待。

> Java 8中的实现
>
> CountDownLatch	
> A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.
>
> CyclicBarrier	
> A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.

#### 3.4 One-Dimensional Iterative Averaging

[Stencil codes](https://en.wikipedia.org/wiki/Stencil_code)

```java
//串行，共n steps
for (iter: [0:nsteps-1]) {
    //并行 n-1 tasks created，
  forall (i: [1:n-1]) {
    newX[i] = (oldX[i-1] + oldX[i+1]) / 2;
  }
  swap pointers newX and oldX;
}
//创建(n-1)*(n-1) task
```

```java
//并行 n-1 tasks created
forall ( i: [1:n-1]) {
  localNewX = newX; localOldX = oldX;//获取一个引用
    //串行 共 n steps
  for (iter: [0:nsteps-1]) {
    localNewX[i] = (localOldX[i-1] + localOldX[i+1]) / 2;
    NEXT; // Barrier 每一个线程执行到这一步的时候，都暂停
    swap pointers localNewX and localOldX;
  }
}
//创建(n-1)次task + 执行(n-1)*(n-1)次NEXT
```

优点：This is a significant improvement since creating tasks is usually more expensive than performing barrier operations.

#### 3.5  Iteration Grouping/Chunking in Parallel Loops

```java
//创建 n-1 个任务，n >> NumberOfCoreOfComputer 时，非常不利也无必要
forall (i : [0:n-1]) a[i] = b[i] + c[i]

//改进思路：对任务划分分组
forall (g:[0:ng-1])
  for (i : mygroup(g, ng, [0:n-1])) a[i] = b[i] + c[i]
  //分组方式1，分块，chunck 比如[1,10] [11,20] [21,30]
  //分组方式2，循环，cyclic 比如[1,4,7,10] [2,5,8,11] [3,6,9,12]
  //具体可以使用PCDP的方法
```

For convenience, the PCDP library provides helper methods, *forallChunked()* and *forall2dChunked()*, that automatically create one-dimensional or two-dimensional parallel loops, and also perform a block-style iteration grouping/chunking on those parallel loops. An example of using the *forall2dChunked()* API for a two-dimensional parallel loop (as in the matrix multiply example) can be seen in the following Java code sketch:

### Week 4 Fuzzy phasers （**Dataflow Synchronization and Pipelining**）

In this week we explore the functionality of fuzzy phasers. The idea is that if
we have an array which average sum we need to compute, we can split work into
`N` tasks and do it concurrently. The improvement is that a task `i` only has to
for for tasks `i-1` and `i+1` to complete. To achieve that, we use `N` phasers
and we call their methods `arrive()` and `awaitAdvance(int phase)`. Notice here
since we run multiple iterations over the same array, this `phase` number keeps
growing and it's crucially important to keep track of the current phase.

``` text
    1 2 3
     \ /
    1 2 3
```

This illustrates that task 2 depends on completion of tasks 1 and 3.

#### 4.1 Split-phase Barriers with Java Phasers

```java
// initialize phaser ph	for use by n tasks ("parties") 
Phaser ph = new Phaser(n);
// Create forall loop with n iterations that operate on ph 
forall (i : [0:n-1]) {
  print HELLO, i;
  int phase = ph.arrive(); //保证所有并行线程都打印 hello后，才会越过barrier
  
  myId = lookup(i); // convert int to a string

  ph.awaitAdvance(phase);//多个并行的线程阻塞在这个位置
  print BYE, myId;
}
```

#### 4.2 Point-to-Point Sychronization with Phasers

![javaPhaser并行优化](/javaPhaser并行优化.png)

#### *4.3 One-Dimensional Iterative Averaging with Phasers

```java
// Allocate array of phasers
Phaser[] ph = new Phaser[n+2]; //array of phasers
for (int i = 0; i < ph.length; i++) ph[i] = new Phaser(1);

// Main computation 
forall ( i: [1:n-1]) {
  for (iter: [0:nsteps-1]) {
    newX[i] = (oldX[i-1] + oldX[i+1]) / 2;
    ph[i].arrive();
    //效果：forall产生的多个并行线程，统一步调，全部算完第一轮后，再进入第二轮
    if (index > 1) ph[i-1].awaitAdvance(iter);
    if (index < n-1) ph[i + 1].awaitAdvance(iter); 
    swap pointers newX and oldX;
  }
}
```

```java
// Allocate array of phasers proportional to number of chunked tasks 
Phaser[] ph = new Phaser[tasks+2]; //array of phasers
for (int i = 0; i < ph.length; i++) ph[i] = new Phaser(1);

// Main computation 
forall ( i: [0:tasks-1]) {
  for (iter: [0:nsteps-1]) {
    //每个线程负责的chunck的边界是有关联的，只要先把边界搞定，剩下的块内部的部分可以异步计算
    // Compute leftmost boundary element for group
    int left = i * (n / tasks) + 1;
    myNew[left] = (myVal[left - 1] + myVal[left + 1]) / 2.0;
    
    // Compute rightmost boundary element for group 
    int right = (i + 1) * (n / tasks);
    myNew[right] = (myVal[right - 1] + myVal[right + 1]) / 2.0;
    
    // Signal arrival on phaser ph AND LEFT AND RIGHT ELEMENTS ARE AV 
    // 此线程标记第 index 次同步中自己负责的部分的边界已经计算好了
    int	index = i + 1;
    ph[index].arrive();
    
    // Compute interior elements in parallel with barrier 
    // 异步执行自己负责的chunck
    for (int j = left + 1; j <= right - 1; j++)
      myNew[j] = (myVal[j - 1] + myVal[j + 1]) / 2.0;
    // Wait for previous phase to complete before advancing 
    if (index > 1) ph[index - 1].awaitAdvance(iter);
    if (index < tasks) ph[index + 1].awaitAdvance(iter);
    swap pointers newX and oldX;
  }
}
```

#### 4.4 Pipeline Parallelism

```java
// Code for pipeline stage i
while ( there is an input to be processed ) {
  // wait for previous stage, if any 
  if (i > 0) ph[i - 1].awaitAdvance(); 
  
  process input;
  
  // signal next stage
  ph[i].arrive();
}
```

#### 4.5 **Data Flow Parallelism**

```java
async( () -> {/* Task A */; A.put(); } ); // Complete task and trigger event A
async( () -> {/* Task B */; B.put(); } ); // Complete task and trigger event B
asyncAwait(A, () -> {/* Task C */} );	    // Only execute task after event A is triggered 
asyncAwait(A, B, () -> {/* Task D */} );	  // Only execute task after events A, B are triggered 
asyncAwait(B, () -> {/* Task E */} );	    // Only execute task after event B is triggered

//等价于
asyncAwait(A, () -> {/* Task C */} );	    // Only execute task after event A is triggered 
asyncAwait(A, B, () -> {/* Task D */} );	  // Only execute task after events A, B are triggered 
asyncAwait(B, () -> {/* Task E */} );    	// Only execute task after event B is triggered 
async( () -> {/* Task A */; A.put(); } ); // Complete task and trigger event A
async( () -> {/* Task B */; B.put(); } ); // Complete task and trigger event B
```



## Concurrent programming in Java

### Week 1 Locks

In this week we were introduced to locks, the basic primitive for
synchronization. Java's `synchronized` keyword is pretty much the same lock that
guards the whole method. Lock, however, can be made more granular. In this week
we applied `ReadWrite` lock to benefit from the fact that read only methods can
have concurrent access to the data, while read-and-write methods should be
guarded by a lock.

### Week 2 Isolation

This week we are introduced to the concepts of critical sections, atomic
variables and isolation. Being a more high-level synchronization construct,
isolation allows for simpler semantics providing the same or even better level
of efficiency. To make things faster in the homework, we use PCDP library and
object-level isolation, not global isolation.

### Week 3 Actors

This week we are introduced to the concept of actors. To explain it in my own
words, actors are independent units of computation that you can communicate with
using `send` method. In the miniproject we need to implement the famous Sieve of
Eratosthenes using actors. To do that, we create a chain of actors that
one-by-one check whether a given number is divisible by the range of numbers,
assigned to the given actor. To make it more efficient, a single actor is
assigned a bunch of numbers, e.g. 1000, not just one.

### Week 4

This week we are introduced to concurrent data structures. In the miniproject we
need to implement Boruvka's algorithm that builds Minimum Weighted Spanning Tree
(MST). The implementation should use granular locks (one per node) to secure
access from multiple threads.

## Distributed programming in Java

### Week 1 Distributed Map Reduce

In this week we need to calculate PageRank using Spark tools to split data and
process it by chunks, a more advanced Map-Reduce model.

### Week 2 Client-server programming

In this week we are introduced to the concept of client-server networking,
specifically to sockets. We need to implement a simple file server that serves
file over a socket by HTTP protocol.

### Week 3 Message passing

This week we are using MPI to distribute matrix calculation. We send the whole
two matrices to all available instances and on each instance we calculate only
those rows that correspond to the current instance. When we're done, we send
results back and collect the resulting matrix.

### Week 4 Combining distribution and multi-threading

This week is basically the same as week 2 but now we need to execute each
request in a separate thread.

## Author

2017-2018, Yuri Bochkarev
